{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangQA â€“ Language-powered question and answer system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import pipeline, TrainingArguments\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "https://huggingface.co/datasets/nlpie/Llama2-MedTuned-Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('nlpie/Llama2-MedTuned-Instructions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train'].select(indices=range(1000))\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the lines to test the model\n",
    "test_data = dataset['train'].select(indices=range(1000, 1200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the format of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    data = dataset['train'][i]\n",
    "    print(f\"Data point {i + 1}:\")\n",
    "    print(\"Instruction:\", data['instruction'])\n",
    "    print(\"Input:\", data['input'])\n",
    "    print(\"Output:\", data['output'])\n",
    "    print(\"\\n-----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating the Creation of Prompts for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a function that takes a dictionary named sample\n",
    "def create_prompt(sample):\n",
    "\n",
    "    # Defines a pre_prompt string that serves as a template for the first part of the prompt\n",
    "    pre_prompt = \"\"\"[INST]<<SYS>> {instruction}\\n\"\"\"\n",
    "\n",
    "    # Concatenates pre_prompt with additional strings to form the complete prompt\n",
    "    prompt = pre_prompt + \"{input}\" +\"[/INST]\"+\"\\n{output}\"\n",
    "\n",
    "    # Assigns the value of the 'instruction' key of the dictionary sample to the variable example_instruction\n",
    "    example_instruction = sample['instruction']\n",
    "\n",
    "    # Assigns the value of the 'input' key of the dictionary sample to the variable example_input\n",
    "    example_input = sample['input']\n",
    "\n",
    "    # Assigns the value of the 'output' key of the dictionary sample to the variable example_output\n",
    "    example_output = sample['output']\n",
    "\n",
    "    # Creates an instance of PromptTemplate with the previously defined prompt and input variables\n",
    "    prompt_template = PromptTemplate(template = prompt,\n",
    "    input_variables = [\"instruction\", \"input\", \"output\"])\n",
    "\n",
    "    # Uses the format method of the prompt_template instance to replace the variables\n",
    "    # in the template with the specified values\n",
    "    unique_prompt = prompt_template.format(instruction = example_instruction,\n",
    "                                          input = example_input,\n",
    "                                          output = example_output)\n",
    "\n",
    "    # Returns the formatted prompt\n",
    "    return [unique_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "prompt = create_prompt(train_data[0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PT-BR**\n",
    "\n",
    "O processo de quantizaÃ§Ã£o serve para reduzir o tamanho do modelo e melhorar a eficiÃªncia computacional, tornando-o mais rÃ¡pido e acessÃ­vel a execuÃ§Ã£o em mÃ¡quinas com hardware de menor capacidade. A quantizaÃ§Ã£o reduz a precisÃ£o dos pesos e ativaÃ§Ãµes do modelo, que geralmente estÃ£o representados em pontos flutuantes de 32 bits (FP32), para formatos de menor precisÃ£o, como int8, int4 e atÃ© mesmo int2.\n",
    "\n",
    "Principais BenefÃ­cios da QuantizaÃ§Ã£o:\n",
    "* ReduÃ§Ã£o do uso de memÃ³ria: Modelos quantizados ocupam menos espaÃ§o na RAM, tornando possÃ­vel a execuÃ§Ã£o em mÃ¡quinas com menos memÃ³ria, CPUs e GPUs menos potentes;\n",
    "* AceleraÃ§Ã£o do tempo de inferÃªncia: OperaÃ§Ãµes com nÃºmeros inteiros sÃ£o mais rÃ¡pidas do que operaÃ§Ãµes com pontos flutuantes, reduzindo a latÃªncia de inferÃªncia;\n",
    "* Menor consumo de energia: Como a quantizaÃ§Ã£o exige menos processamento de ponto flutuante, o gasto energÃ©tico Ã© menor, tornando possÃ­vel a execuÃ§Ã£o em dispositivos embarcados e em dispositivos mÃ³veis. \n",
    "\n",
    "---\n",
    "**EN-US**\n",
    "\n",
    "The quantization process serves to reduce the size of the model and improve computational efficiency, making it faster and more accessible to run on machines with lower-capacity hardware. Quantization reduces the precision of the model's weights and activations, which are usually represented in 32-bit floating point (FP32) formats, to lower-precision formats, such as int8, int4, and even int2.\n",
    "\n",
    "Main Benefits of Quantization:\n",
    "* Reduced memory usage: Quantized models take up less space in RAM, making it possible to run on machines with less memory, less powerful CPUs and GPUs;\n",
    "* Accelerated inference time: Operations with integers are faster than operations with floating points, reducing inference latency;\n",
    "* Lower energy consumption: Since quantization requires less floating point processing, energy expenditure is lower, making it possible to run on embedded devices and mobile devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bitsandbytes Config\n",
    "\n",
    "The `BitsAndBytesConfig` object from the **bitsandbytes** package is used to configure **model quantization** during loading. This quantization reduces the model's weight size and **makes it possible to train and infer large LLMs on smaller GPUs**, such as those with 16 GB or 24 GB of VRAM. \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Explanation of the Parameters**\n",
    "#### `load_in_4bit=True`\n",
    "- **What it does:** Enables **4-bit** quantization to reduce VRAM usage.  \n",
    "- **Alternative:** `load_in_8bit=True`, which uses **8-bit** quantization instead of 4-bit.  \n",
    "- **Why use it:** 4-bit models take up **half** the memory of an 8-bit model but may lose some precision.  \n",
    "\n",
    "---\n",
    "\n",
    "#### `bnb_4bit_quant_type=\"nf4\"`\n",
    "- **What it does:** Defines the quantization type used to store weights.  \n",
    "- **Available options:**  \n",
    "  - `\"fp4\"` â†’ **Float4**, a 4-bit floating-point format.  \n",
    "  - `\"nf4\"` â†’ **Normal Float4**, an optimized version of Float4 that improves precision.  \n",
    "- **Why use `\"nf4\"`:** This format has been optimized for **AI models**, providing better precision than `\"fp4\"` while reducing weight size.  \n",
    "\n",
    "P.S.: The **NF4 (Normal Float 4)** format improves precision compared to **FP4 (Float 4)** because it uses a **non-linear distribution** of representable values, optimizing bit allocation to represent numbers that occur more frequently in AI models. It was specifically designed for **Deep Learning**, especially to handle **LLMs**.\n",
    "\n",
    "---\n",
    "\n",
    "#### `bnb_4bit_compute_dtype=\"float16\"`\n",
    "- **What it does:** Sets the data type used for computations during training/inference.  \n",
    "- **Available options:**  \n",
    "  - `\"float16\"` â†’ Uses `torch.float16`, good for GPUs that support 16-bit calculations.  \n",
    "  - `\"bfloat16\"` â†’ Uses `torch.bfloat16`, better for modern GPUs like A100/H100.  \n",
    "  - `\"float32\"` â†’ Uses `torch.float32`, offering higher precision but consuming more memory.  \n",
    "- **Why use `\"float16\"`:** Most GPUs support `float16`, which balances precision and efficiency. If using GPUs like A100 or H100, `bfloat16` might be a better choice.  \n",
    "\n",
    "---\n",
    "\n",
    "#### `bnb_4bit_use_double_quant=False`\n",
    "- **What it does:** Controls whether **double quantization** will be used.  \n",
    "- **Available options:**  \n",
    "  - `False` â†’ Only a single quantization is applied.  \n",
    "  - `True` â†’ **Applies a second quantization** to the already quantized weights.  \n",
    "- **Why use `False`:** If your GPU has **enough memory** (e.g., 24 GB), double quantization **may not be necessary**. If you want to **save even more VRAM**, you can try `True`.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Other Possible Configurations**\n",
    "Here are some variations you can test, depending on your hardware and goals:\n",
    "\n",
    "#### ðŸ”¸ **For maximum memory efficiency**\n",
    "```python\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",  # Uses less memory on modern GPUs\n",
    "    bnb_4bit_use_double_quant=True      # Enables double quantization to save VRAM\n",
    ")\n",
    "```\n",
    "**Recommended use:** If running a very large model on a GPU with **limited VRAM** (e.g., RTX 3090, 4090, or A100 with 40 GB).\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¸ **For better precision during inference**\n",
    "```python\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Uses 8-bit instead of 4-bit\n",
    "    bnb_4bit_compute_dtype=\"float32\"  # Uses float32 for more precise calculations\n",
    ")\n",
    "```\n",
    "**Recommended use:** When you **don't need to save much memory** and want a **more accurate model**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Conclusion**\n",
    "Using `BitsAndBytesConfig` is essential for **running large models on limited hardware**. Here's a quick summary of the most important parameters:\n",
    "\n",
    "| Parameter | Function | Common Values | When to Use |\n",
    "|-----------|--------|---------------|------------|\n",
    "| `load_in_4bit` | Enables 4-bit quantization | `True` or `False` | For maximum memory savings |\n",
    "| `bnb_4bit_quant_type` | Type of quantization | `\"fp4\"` or `\"nf4\"` | `\"nf4\"` for better precision |\n",
    "| `bnb_4bit_compute_dtype` | Data type for computations | `\"float16\"`, `\"bfloat16\"`, `\"float32\"` | `\"bfloat16\"` for modern GPUs |\n",
    "| `bnb_4bit_use_double_quant` | Enables double quantization | `True` or `False` | `True` if VRAM is limited |\n",
    "\n",
    "If you want more efficiency, you can test different combinations and check VRAM consumption using `torch.cuda.memory_allocated()`. ðŸš€  \n",
    "\n",
    "If you have any more questions or need adjustments for a specific hardware setup, let me know! ðŸ”¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables loading of the base model with 4-bit precision\n",
    "use_4bit = True\n",
    "\n",
    "# Sets the dtype for the base model\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Disables double quantization\n",
    "use_nested_quant = False\n",
    "\n",
    "# Sets the dtype for computation in PyTorch\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the config\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit = use_4bit,\n",
    "                                bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "                                bnb_4bit_compute_dtype = compute_dtype,\n",
    "                                bnb_4bit_use_double_quant = use_nested_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying if the GPU supports bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"The GPU suporrts bfloat16. You can accelerate the train using bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the LLM and the Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/NousResearch/Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "# llm_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "llm_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "\n",
    "# Load the base model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_name,\n",
    "                                              quantization_config = bnb_config,\n",
    "                                            #   trust_remote_code=True,\n",
    "                                              device_map = \"auto\",\n",
    "                                              use_cache = False\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the EOS token from the tokenizer to pad at the end of each sequence\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Enable padding at the end of each sentence\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring LoRa Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization represents data with fewer bits, making it a useful technique for reducing memory usage and speeding up inference, especially in the context of LLMs.  \n",
    "\n",
    "Once a model is quantized, it is typically not trained **directly** for downstream tasks because training can become unstable due to the reduced precision of weights and activations. However, since PEFT methods only add extra trainable parameters, this allows for training a quantized model with a PEFT adapter on top! Combining quantization with PEFT can be a good strategy to train even the largest models on a single GPU. For example, QLoRA is a method that quantizes a model to 4 bits and then trains it with LoRA. This method enables fine-tuning a 65B parameter model on a single 48GB GPU, for instance.  \n",
    "\n",
    "The goal of PEFT (Parameter-Efficient Fine-Tuning) is to keep most of the pre-trained model's parameters fixed while adjusting only a small subset of parameters to adapt the model to a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRa Parameters\n",
    "peft_config = LoraConfig(r = 8,\n",
    "                        lora_alpha = 16,\n",
    "                        lora_dropout = 0.05,\n",
    "                        bias = \"none\",\n",
    "                        task_type = \"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model to train\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the quantized model with the LoRa adapters\n",
    "model = get_peft_model(model, peft_config=peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = 'adjusted_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train arguments\n",
    "training_arguments = TrainingArguments(output_dir = output_model,\n",
    "                                       per_device_train_batch_size = 1,\n",
    "                                       gradient_accumulation_steps = 4,\n",
    "                                       optim = \"paged_adamw_32bit\",\n",
    "                                       learning_rate = 2e-4,\n",
    "                                       lr_scheduler_type = \"cosine\",\n",
    "                                       save_strategy = \"epoch\",\n",
    "                                       logging_steps = 10,\n",
    "                                       num_train_epochs = 3,\n",
    "                                       max_steps = 150,\n",
    "                                       fp16 = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force the model to allocate memory correctly\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the Trainer\n",
    "# Optimized for fine-tuning pre-trained models with smaller datasets on supervised learning tasks.\n",
    "trainer = SFTTrainer(model = model,\n",
    "                     peft_config = peft_config,\n",
    "                    #  max_seq_length = 512,\n",
    "                     tokenizer = tokenizer,\n",
    "                    #  packing = True,\n",
    "                     formatting_func = create_prompt,\n",
    "                     args = training_arguments,\n",
    "                     train_dataset = train_data,\n",
    "                     eval_dataset = test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model save\n",
    "trainer.save_model('final_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the pipeline of Text Generation with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pre-prompt with the instruction\n",
    "pre_prompt = \"\"\"[INST] <<SYS>>\\nAnalyze the question and answer with the best option.\\n\"\"\"\n",
    "\n",
    "# Create the prompt adding the input\n",
    "prompt = pre_prompt + \"Here is my question {context}\" + \"[\\INST]\"\n",
    "\n",
    "# Create the prompt template with LangChain\n",
    "prompt = PromptTemplate(template = prompt, input_variables=[\"context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract away most of the complex code in the library, providing a simple API dedicated to a variety of tasks, including named entity recognition, masked language modeling, sentiment analysis, feature extraction, and question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline object\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                 model = merged_model,\n",
    "                 tokenizer = tokenizer,\n",
    "                 max_new_tokens = 512,\n",
    "                 use_cache = False,\n",
    "                 do_sample = True,\n",
    "                 pad_token_id = tokenizer.eos_token_id,\n",
    "                 top_p = 0.7,\n",
    "                 temperature = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Hugging Face Pipeline\n",
    "llm_pipeline = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the memory\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM Chain\n",
    "chat_llm_chain = LLMChain(llm = llm_pipeline,\n",
    "                          prompt = prompt,\n",
    "                          verbose = False,\n",
    "                          memory = memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Model and Using the Question and Answer System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '''###Question: All of the following provisions are included in the Primary health care according to the Alma Ata declaration except:\n",
    "###Options:\n",
    "A. Adequate supply of safe drinking water\n",
    "B. Nutrition\n",
    "C. Provision of free medicines\n",
    "D. Basic sanitation'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = chat_llm_chain.predict(context = context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
