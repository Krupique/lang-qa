{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangQA – Language-powered question and answer system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import pipeline, TrainingArguments\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "https://huggingface.co/datasets/nlpie/Llama2-MedTuned-Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 200252/200252 [00:00<00:00, 1305596.68 examples/s]\n",
      "Generating validation split: 100%|██████████| 70066/70066 [00:00<00:00, 1771353.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('nlpie/Llama2-MedTuned-Instructions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'source'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = dataset['train'].select(indices=range(1000))\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the lines to test the model\n",
    "test_data = dataset['train'].select(indices=range(1000, 1200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the format of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data point 1:\n",
      "Instruction: In your role as a medical professional, address the user's medical questions and concerns.\n",
      "Input: My relative suffering from secondary lever cancer ( 4th stage as per Allopathic doctor) and primary is in rectum. He is continuously with 103 to 104 degree F fever. Allpathic doctor suggested chemo only after fever subsidises. Is treatment possible at Lavanya & what is the time scale of recover.\n",
      "Output: Hi, dairy have gone through your question. I can understand your concern. He has rectal cancer with liver metastasis. It is stage 4 cancer. Surgery is not possible at this stage. Only treatment options are chemotherapy and radiotherapy according to type of cancer. Inspite of all treatment prognosis is poor. Life expectancy is not good. Consult your doctor and plan accordingly. Hope I have answered your question, if you have any doubts then contact me at bit.ly/ Chat Doctor. Thanks for using Chat Doctor. Wish you a very good health.\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Data point 2:\n",
      "Instruction: Your role as a doctor requires you to answer the medical questions taking into account the patient's description.\n",
      "Analyze the question given its context. Give both long answer and yes/no decision.\n",
      "Input: ###Question: Are fibrocytes involved in inflammation as well as fibrosis in the pathogenesis of Crohn 's disease?\n",
      "###Context: We previously showed that fibrocytes, a hematopoietic stem cell source of fibroblasts/myofibroblasts, infiltrated the colonic mucosa of a murine colitis model. We investigated whether fibrocytes were involved in the pathogenesis of Crohn's disease. Human surgical intestinal specimens were stained with anti-leukocyte-specific protein 1 and anti-collagen type-I (ColI) antibodies. Circulating fibrocytes in the human peripheral blood were quantified by fluorescence-activated cell sorting with anti-CD45 and anti-ColI antibodies. Cultured human fibrocytes were prepared by culturing peripheral CD14(+) monocytes. In the specimens of patients with Crohn's disease, the fibrocyte/total leukocyte percentage was significantly increased in inflammatory lesions (22.2 %, p < 0.01) compared with that in non-affected areas of the intestine (2.5 %). Interestingly, the percentage in fibrotic lesions was similar (2.2 %, p = 0.87) to that in non-affected areas. The percentages of circulating fibrocytes/total leukocytes were significantly higher in patients with Crohn's disease than in healthy controls. Both CXC-chemokine receptor 4(+) and intercellular adhesion molecule 1(+) fibrocyte numbers were significantly increased in Crohn's disease, suggesting that circulating fibrocytes have a higher ability to infiltrate injured sites and traffic leukocytes. In cultured fibrocytes, lipopolysaccharide treatment remarkably upregulated tumor necrosis factor (TNF)-α mRNA (17.0 ± 5.7-fold) and ColI mRNA expression (12.8 ± 5.7-fold), indicating that fibrocytes stimulated by bacterial components directly augmented inflammation as well as fibrosis.\n",
      "Output: Fibrocytes are recruited early in the inflammatory phase and likely differentiate into fibroblasts/myofibroblasts until the fibrosis phase. They may enhance inflammation by producing TNF-α and can directly augment fibrosis by producing ColI.\n",
      "\n",
      "###Answer: yes\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Data point 3:\n",
      "Instruction: Your identity is a doctor, kindly provide answers to the medical questions with consideration of the patient's description.\n",
      "Analyze the question and answer with the best option.\n",
      "Input: ###Question: Afterhyperpolarization due to\n",
      "###Options:\n",
      "A. Na efflux\n",
      "B. Na+ influx\n",
      "C. CI influx\n",
      "D. K+ efflux\n",
      "\n",
      "Output: ###Rationale: Slow return of the K+ channels to the closed state thus K+ efflux.(Ref: Textbook of physiology AK Jain 5th edition page no.36)\n",
      "\n",
      "###Answer: OPTION D IS CORRECT.\n",
      "\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    data = dataset['train'][i]\n",
    "    print(f\"Data point {i + 1}:\")\n",
    "    print(\"Instruction:\", data['instruction'])\n",
    "    print(\"Input:\", data['input'])\n",
    "    print(\"Output:\", data['output'])\n",
    "    print(\"\\n-----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating the Creation of Prompts for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a function that takes a dictionary named sample\n",
    "def create_prompt(sample):\n",
    "\n",
    "    # Defines a pre_prompt string that serves as a template for the first part of the prompt\n",
    "    pre_prompt = \"\"\"[INST]<<SYS>> {instruction}\\n\"\"\"\n",
    "\n",
    "    # Concatenates pre_prompt with additional strings to form the complete prompt\n",
    "    prompt = pre_prompt + \"{input}\" +\"[/INST]\"+\"\\n{output}\"\n",
    "\n",
    "    # Assigns the value of the 'instruction' key of the dictionary sample to the variable example_instruction\n",
    "    example_instruction = sample['instruction']\n",
    "\n",
    "    # Assigns the value of the 'input' key of the dictionary sample to the variable example_input\n",
    "    example_input = sample['input']\n",
    "\n",
    "    # Assigns the value of the 'output' key of the dictionary sample to the variable example_output\n",
    "    example_output = sample['output']\n",
    "\n",
    "    # Creates an instance of PromptTemplate with the previously defined prompt and input variables\n",
    "    prompt_template = PromptTemplate(template = prompt,\n",
    "    input_variables = [\"instruction\", \"input\", \"output\"])\n",
    "\n",
    "    # Uses the format method of the prompt_template instance to replace the variables\n",
    "    # in the template with the specified values\n",
    "    prompt_unico = prompt_template.format(instruction = example_instruction,\n",
    "                                          input = example_input,\n",
    "                                          output = example_output)\n",
    "\n",
    "    # Returns the formatted prompt\n",
    "    return prompt_unico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>> In your role as a medical professional, address the user's medical questions and concerns.\n",
      "My relative suffering from secondary lever cancer ( 4th stage as per Allopathic doctor) and primary is in rectum. He is continuously with 103 to 104 degree F fever. Allpathic doctor suggested chemo only after fever subsidises. Is treatment possible at Lavanya & what is the time scale of recover.[/INST]\n",
      "Hi, dairy have gone through your question. I can understand your concern. He has rectal cancer with liver metastasis. It is stage 4 cancer. Surgery is not possible at this stage. Only treatment options are chemotherapy and radiotherapy according to type of cancer. Inspite of all treatment prognosis is poor. Life expectancy is not good. Consult your doctor and plan accordingly. Hope I have answered your question, if you have any doubts then contact me at bit.ly/ Chat Doctor. Thanks for using Chat Doctor. Wish you a very good health.\n"
     ]
    }
   ],
   "source": [
    "# Testing the function\n",
    "prompt = create_prompt(train_data[0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables loading of the base model with 4-bit precision\n",
    "use_4bit = True\n",
    "\n",
    "# Sets the dtype for the base model\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Disables double quantization\n",
    "use_nested_quant = False\n",
    "\n",
    "# Sets the dtype for computation in PyTorch\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the config\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit = use_4bit,\n",
    "                                bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "                                bnb_4bit_compute_dtype = compute_dtype,\n",
    "                                bnb_4bit_use_double_quant = use_nested_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "The GPU suporrts bfloat16. You can accelerate the train using bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Verifying if the GPU supports bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"The GPU suporrts bfloat16. You can accelerate the train using bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the LLM and the Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/NousResearch/Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# LLM\n",
    "llm_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "\n",
    "# Load the base model with quantization\n",
    "modelo = AutoModelForCausalLM.from_pretrained(llm_name,\n",
    "                                              quantization_config = bnb_config,\n",
    "                                              device_map = \"auto\",\n",
    "                                              use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the EOS token from the tokenizer to pad at the end of each sequence\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Enable padding at the end of each sentence\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
